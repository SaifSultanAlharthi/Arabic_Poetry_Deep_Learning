{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kg0E0ihyFHbK"
      },
      "source": [
        "First let's load in our required libraries for data loading and model creation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-wlmnfmFHbU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPe5pHvDFHbY"
      },
      "source": [
        "## Load in Data\n",
        "\n",
        "Then, we'll load the Arabic poems text file and convert it into pre-process for our network to use. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLX1Zv6ZFHba"
      },
      "outputs": [],
      "source": [
        "# open text file and read in data as `text`\n",
        "with open('nor.txt', 'r',encoding=\"utf-8\") as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZeJ9oX5FHbc"
      },
      "source": [
        "Let's check out the first 100 characters, make sure everything is peachy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OpU9s9GhFHbe",
        "outputId": "dd0954f0-58fa-401c-8231-66c4e2e18fdf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'يابير طيك طي واعماقك اعماق\\nويعيش بك داب وحمامه وعصفور\\nفاضت عيونك ماك وابتلت احداق\\nوتنهدت منك محاله ا'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "text[:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkwBzCQcFHbg"
      },
      "source": [
        "It looks like a typical Arabic poem structure, Yay!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2a7vuodFHbj"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "In the cells, below, I'm creating a couple **dictionaries** to convert the characters to and from integers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxwXovuqFHbm"
      },
      "outputs": [],
      "source": [
        "# encode the text and map each character to an integer and vice versa\n",
        "\n",
        "# we create two dictionaries:\n",
        "# 1. int2char, which maps integers to characters\n",
        "# 2. char2int, which maps characters to unique integers\n",
        "chars = tuple(set(text))\n",
        "char2int = dict(enumerate(chars))\n",
        "int2char = {ch: ii for ii, ch in char2int.items()}\n",
        "\n",
        "# encode the text\n",
        "encoded = np.array([int2char[ch] for ch in text])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtTJfCkEFHbp"
      },
      "source": [
        "Now, we can see the above part of the poem encoded in numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAPE_BUlFHbq",
        "outputId": "36276c67-6584-4a7d-bcf2-48177da9a6e9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([40, 20, 36, 40, 44, 29, 43, 40, 30, 29, 43, 40, 29, 11, 20,  2,  3,\n",
              "       20, 19, 30, 29, 20,  2,  3, 20, 19, 35, 11, 40,  2, 40, 22, 29, 36,\n",
              "       30, 29,  7, 20, 36, 29, 11, 26,  3, 20,  3, 46, 29, 11,  2, 41, 33,\n",
              "       11, 44, 35, 33, 20,  5, 28, 29,  2, 40, 11, 25, 30, 29,  3, 20, 30,\n",
              "       29, 11, 20, 36, 28, 39, 28, 29, 20, 26,  7, 20, 19, 35, 11, 28, 25,\n",
              "       46,  7, 28, 29,  3, 25, 30, 29,  3, 26, 20, 39, 46, 29, 20])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "encoded[:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmOyg17PFHbs"
      },
      "source": [
        "## Pre-processing the data\n",
        "\n",
        "in char-RNN, the LSTM expects an input that is **one-hot encoded**\n",
        "for more about one-hot-encoding, check out this post https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5CvfBvcFHbu"
      },
      "outputs": [],
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "    \n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "    \n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "    \n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    \n",
        "    return one_hot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqF3xx7jFHbv"
      },
      "source": [
        "## Making training mini-batches\n",
        "\n",
        "\n",
        "To train on this data, we also want to create mini-batches for training. \n",
        "<img src=\"assets/sequence_batching@1x.png\" width=500px>\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "In this example, we'll take the encoded characters (passed in as the `arr` parameter) and split them into multiple sequences, given by `batch_size`. Each of our sequences will be `seq_length` long.\n",
        "\n",
        "### Creating Batches\n",
        "\n",
        "1. The first thing we need to do is discard some of the text so we only have completely full mini-batches. \n",
        "\n",
        "\n",
        "2. After that, we need to split `arr` into $N$ batches. \n",
        "\n",
        "\n",
        "3. Now that we have this array, we can iterate through it to get our mini-batches. \n",
        "\n",
        "\n",
        "### Train and test batches\n",
        "4. create two batches x,y where x is the input batch and y is the training batch which is exactly x but shifted with one character\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtbIzDY_FHbx"
      },
      "outputs": [],
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "\n",
        "    batch_size_total = batch_size * seq_length\n",
        "    # total number of batches we can make\n",
        "    n_batches = len(arr)//batch_size_total\n",
        "    \n",
        "    # Keep only enough characters to make full batches\n",
        "    arr = arr[:n_batches * batch_size_total]\n",
        "    # Reshape into batch_size rows\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "    \n",
        "    # iterate through the array, one sequence at a time\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        # The features\n",
        "        x = arr[:, n:n+seq_length]\n",
        "        # The targets, shifted by one\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JChtmhJdFHby"
      },
      "source": [
        "### Visualise the output\n",
        "\n",
        "let's try to get batches out of the 100 character of encoded data and see what's happening"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1KmCWv-FHb0",
        "outputId": "425f9309-3f6a-4280-db6c-dfd2eb82dbd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x\n",
            " [[40 20 36 40 44 29 43 40 30 29]\n",
            " [40 29 18 46 21 11  7 29 20 39]\n",
            " [44  5 29  3 21 25 29 13  3 21]\n",
            " [21 36 14 21 44 29  7 20 44 18]\n",
            " [17 31 39 23 29 26 40 25 20 29]\n",
            " [ 3 29  3 21 20 29 40 37 25 21]]\n",
            "\n",
            "y\n",
            " [[20 36 40 44 29 43 40 30 29 43]\n",
            " [29 18 46 21 11  7 29 20 39 25]\n",
            " [ 5 29  3 21 25 29 13  3 21  3]\n",
            " [36 14 21 44 29  7 20 44 18 23]\n",
            " [31 39 23 29 26 40 25 20 29 11]\n",
            " [29  3 21 20 29 40 37 25 21 11]]\n"
          ]
        }
      ],
      "source": [
        "batches = get_batches(encoded, 6, 20)\n",
        "x, y = next(batches)\n",
        "# printing out the first 10 items in a sequence\n",
        "print('x\\n', x[:10, :10])\n",
        "print('\\ny\\n', y[:10, :10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWfDRtnVFHb1"
      },
      "source": [
        "And as we can see that $y$ is the same as $x$ but only shifted with one character "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpOcKp4FFHb1"
      },
      "source": [
        "---\n",
        "## Defining the network with PyTorch\n",
        "\n",
        "Below is where i define the network.\n",
        "\n",
        "<img src=\"assets/charRNN.png\" width=500px >\n",
        "\n",
        "Next, i will use PyTorch to define the architecture of the network. i start by defining the layers and operations we want. Then, defining a method for the forward pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7TQOm9JFHb2",
        "outputId": "8e4eb66a-4393-457b-dba6-3166749eb84f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on GPU!\n"
          ]
        }
      ],
      "source": [
        "# check if GPU is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU!')\n",
        "else: \n",
        "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qQIvGwtFHb3"
      },
      "outputs": [],
      "source": [
        "class CharRNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
        "                               drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        \n",
        "        # creating character dictionaries\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "        \n",
        "        ## define the LSTM\n",
        "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        ## define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        ## define the final, fully-connected output layer\n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "      \n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "        ''' Forward pass through the network. \n",
        "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
        "                \n",
        "        ## Get the outputs and the new hidden state from the lstm\n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "        \n",
        "        ## pass through a dropout layer\n",
        "        out = self.dropout(r_output)\n",
        "        \n",
        "        # Stack up LSTM outputs using view\n",
        "        # you may need to use contiguous to reshape the output\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "        \n",
        "        ## put x through the fully-connected layer\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        # return the final output and the hidden state\n",
        "        return out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "        \n",
        "        return hidden\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bL5npoFlFHb4"
      },
      "outputs": [],
      "source": [
        "def train(net, data, epochs=5, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "    ''' Training a network \n",
        "    \n",
        "        Arguments\n",
        "        ---------\n",
        "        \n",
        "        net: CharRNN network\n",
        "        data: text data to train the network\n",
        "        epochs: Number of epochs to train\n",
        "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "        seq_length: Number of character steps per mini-batch\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction of data to hold out for validation\n",
        "        print_every: Number of steps for printing training and validation loss\n",
        "    \n",
        "    '''\n",
        "    # keep track of training and validation loss\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    valid_loss_min = np.Inf # track change in validation loss\n",
        "\n",
        "\n",
        "    net.train()\n",
        "    \n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "        \n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "            \n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            if(train_on_gpu):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "            \n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "            \n",
        "            # calculate the loss and perform backprop\n",
        "            loss = train_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "            \n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    \n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "                    inputs, targets = x, y\n",
        "                    if(train_on_gpu):\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = valid_loss =  criterion(output, targets.view(batch_size*seq_length).long())\n",
        "                \n",
        "                    val_losses.append(val_loss.item())\n",
        "                \n",
        "                net.train() # reset to train mode after iterationg through validation data\n",
        "                \n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
        "                \n",
        "    # save model if validation loss has decreased\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "        valid_loss_min,\n",
        "        valid_loss))\n",
        "        model_name = 'best_loss_so_far.net'\n",
        "\n",
        "        checkpoint = {'n_hidden': net.n_hidden,\n",
        "                       'n_layers': net.n_layers,\n",
        "                       'state_dict': net.state_dict(),\n",
        "                       'tokens': net.chars}\n",
        "\n",
        "        with open(model_name, 'wb') as f:\n",
        "            torch.save(model_name, f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gz0Dhwg5FHb5"
      },
      "source": [
        "## Training \n",
        "\n",
        "the next 2 cell should define the architecture and  train the model we defined above, the number of $LSTM$ layers **n_layers** is up to you, i trained it with 5 layers on google colab with 50 $epochs$ and it took **4** hours\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcPfxuKnFHb6",
        "outputId": "a5095211-f62f-410a-9d8e-6a42f608ede0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(47, 512, num_layers=5, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=47, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# define and print the net\n",
        "n_hidden=512\n",
        "n_layers=5\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulFozG-xFHb7"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "seq_length = 100\n",
        "n_epochs = 50 # start smaller if you are just testing initial behavior\n",
        "\n",
        "# train the model\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fa12_p6FHb7"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "**Note:** the following architecure with the text file used is estimated to take 375 hours on my machine, so that's why google colab is used here, and iam gonna load my trained model and use it to predict.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUm7ea1AFHb8"
      },
      "source": [
        "## Load the sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amqs6uEjFHcA",
        "outputId": "6d7849e6-2b2a-4723-fabf-b95a4acf84fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "loaded = CharRNN(net.chars,net.n_hidden,net.n_layers)\n",
        "loaded.load_state_dict(net.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XzvLIhgFHcA"
      },
      "source": [
        "The following two functions is taked from **UDACITY - DeepLearning with PyTorch** to sample only the input and make it in correct foramt then take that input and generate output characters as size specified"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTV4s4hXFHcB"
      },
      "outputs": [],
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "        ''' Given a character, predict the next character.\n",
        "            Returns the predicted character and the hidden state.\n",
        "        '''\n",
        "        \n",
        "        # tensor inputs\n",
        "        x = np.array([[net.char2int[char]]])\n",
        "        x = one_hot_encode(x, len(net.chars))\n",
        "        inputs = torch.from_numpy(x)\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        # detach hidden state from history\n",
        "        h = tuple([each.data for each in h])\n",
        "        # get the output of the model\n",
        "        out, h = net(inputs, h)\n",
        "\n",
        "        # get the character probabilities\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # move to cpu\n",
        "        \n",
        "        # get top characters\n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(net.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "        # select the likely next character with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "        \n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return net.int2char[char], h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gY3XWdzYFHcB"
      },
      "outputs": [],
      "source": [
        "def sample(net, size, prime='The', top_k=None):\n",
        "        \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "    \n",
        "    net.eval() # eval mode\n",
        "    \n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "    \n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2u5UfKPFHcD",
        "outputId": "f19131ee-e44d-4375-cc5e-e4828c609ccf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "قلبي لن هقانه  داخ ساقيه صبب يماعبت\n",
            "ويل من حفرب بقي الساقا ووبقى باعب\n",
            "النو سنرير فحع العبح لقوخ ننه تنرول انومي لشلاب تنمر فارابٍ طيي الشاجل تجيق\n",
            "صاوخ فاكحت ابتووك ما أأانا إي\n",
            "ووشهه وامعب الراسا ولر وووح\n",
            "الححدهن\n",
            "يا معلل القجد واسغك وماح قظغد\n",
            "عيح مك افيدت من عبلين أ\n",
            "جلي مهي غمل كحديا قمال\n",
            "نقج بين عفيره يضل يا واسات حصع وعج السلي\n",
            "الصد ما بقوات الموي تناريه معلد لفبل ولا لمفي ديدتاماج يعكد الطوره\n",
            "و\n",
            "اتيه تسرت مثر حل المماد من قف\n",
            "قال لا زببكا وجمرده ما صلضك وشيهك وانلست\n",
            "اقلي اش الغيوخ ثفر اللالن ابال\n",
            "ماهد\n"
          ]
        }
      ],
      "source": [
        "print(sample(loaded, 500, prime='قلبي', top_k=40))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEs1L3rSFHcE"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of TheDreamBeCameTrue.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}